{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!nvidia-smi","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-10-13T16:46:56.526454Z","iopub.execute_input":"2022-10-13T16:46:56.527739Z","iopub.status.idle":"2022-10-13T16:46:57.506787Z","shell.execute_reply.started":"2022-10-13T16:46:56.527668Z","shell.execute_reply":"2022-10-13T16:46:57.505643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport torch\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport torchvision.transforms as transforms\n\nfrom torch import nn\nfrom torchvision.transforms import ToTensor, Lambda, ToPILImage, RandomRotation\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision.io import read_image\n\nfrom resnet_pure import BasicBlock, ResNet, Bottleneck\nfrom my_utils import plot, saving_model\nfrom places365 import PlacesDataset\nfrom report import report, export_figure\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(f\"Using {device} for model.\")\nprint(\"-\" * 100)","metadata":{"execution":{"iopub.status.busy":"2022-10-13T16:46:57.509015Z","iopub.execute_input":"2022-10-13T16:46:57.509735Z","iopub.status.idle":"2022-10-13T16:46:58.384532Z","shell.execute_reply.started":"2022-10-13T16:46:57.509678Z","shell.execute_reply":"2022-10-13T16:46:58.383409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transform = transforms.Compose([\n    transforms.ToPILImage(), # Must convert to PIL image for subsequent operations to run\n#     transforms.RandomRotation(0), # Image augmentation\n    transforms.Resize((64, 64)), # Original image have 256x256\n    transforms.ToTensor() # Must convert to pytorch tensor for subsequent operations to run\n])\n\nbatch_size = 128\n\ntrain_set_config = {'train': True,\n                    'train_version': 2,\n                    'transform': transform}\n\nval_set_config = {'train': False,\n                  'transform': transform}\n\ndataloader_params = {'batch_size': batch_size, 'shuffle': True, 'num_workers': 0}\n\ntrain_set = PlacesDataset(**train_set_config)\ntrain_dataloader = DataLoader(dataset=train_set, **dataloader_params)\n\nval_set = PlacesDataset(**val_set_config)\nval_dataloader = DataLoader(dataset=val_set, **dataloader_params)\n\n# Initialize hyper parameters\nlearning_rate = 1e-3\nepoch = 30\n\n# Initialize model\nmodel = ResNet(BasicBlock, [2, 2, 2, 2], num_classes=50).to(device)\nmodel_name = f'ResNet50_{epoch}ep_{learning_rate}lr_{batch_size}bsize_{len(train_set)}size.pth'\ndataloaders = [train_dataloader, val_dataloader]","metadata":{"execution":{"iopub.status.busy":"2022-10-13T16:46:58.385946Z","iopub.execute_input":"2022-10-13T16:46:58.38685Z","iopub.status.idle":"2022-10-13T16:47:02.136838Z","shell.execute_reply.started":"2022-10-13T16:46:58.38681Z","shell.execute_reply":"2022-10-13T16:47:02.135861Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = 'cuda' if torch.cuda.is_available() else 'cpu'\nlog = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n\n        \ndef export_figure(log):\n    pair1 = ['train_loss', 'train_acc']\n    pair2 = ['train_loss', 'val_loss']\n    pair3 = ['val_acc', 'val_loss']\n    pair4 = ['val_acc', 'train_acc']\n    for pair in [pair1, pair2, pair3, pair4]:\n        fig_name = ' vs '.join(pair)\n        plot(log, pair, fig_name)\n        \ndef train_loop(dataloader, model, loss_fn, optimizer):\n    size = len(dataloader.dataset)\n    log_idx = 100\n    train_loss, train_acc = 0, 0\n    for batch, (X, y) in enumerate(dataloader):        \n        # Compute prediction and loss\n        X, y = X.to(device), y.to(device)\n        size_batch = len(y)\n        pred = model(X)\n        loss = loss_fn(pred, y)\n        acc = (pred.argmax(1) == y).type(torch.float).sum().item()\n        \n        # Backpropagation\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        if batch % log_idx == 0:\n            loss, current = loss.item(), batch * len(y)\n            print(f\"Loss: {loss:>0.3f}| Acc: {(acc * 100 / size_batch):>0.3f}%  [{current:>5d}/{size:>5d}]\")\n        \n        train_loss += loss\n        train_acc += acc\n    return torch.mean(train_loss).item(), train_acc * 100 / size\n            \ndef val_loop(dataloader, model, loss_fn):\n    size = len(dataloader.dataset)\n    val_loss, val_acc = 0, 0\n    \n    with torch.no_grad():\n        for batch, (X, y) in enumerate(dataloader):\n            X, y = X.to(device, dtype=torch.float), y.to(device)\n            pred = model(X)\n            val_loss += loss_fn(pred, y).item()\n            val_acc += (pred.argmax(1) == y).type(torch.float).sum().item()\n            \n    val_loss /= size\n    val_acc /= size\n    print(f\"Test Error: \\n Accuracy: {(100 * val_acc):>0.3f}%, Avg loss: {val_loss:>4f} \\n\")\n    return val_loss, val_acc * 100\n\ndef report(model, model_name, dataloaders, learning_rate, epoch, L2=None):\n    # Initialize the loss function and optimizer\n    loss_fn = nn.CrossEntropyLoss().to(device)\n    optimizer = None\n    if L2:\n        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=L2)\n    else:\n        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n\n    train_params = {'dataloader': dataloaders[0],\n                    'model': model,\n                    'loss_fn': loss_fn,\n                    'optimizer': optimizer}\n\n    val_params = {'dataloader': dataloaders[1], \n                  'model': model,\n                  'loss_fn': loss_fn}\n    \n    for ep in range(epoch):\n        print(f\"EPOCH {ep} --------------------------------------------\")\n        train_loss, train_acc = train_loop(**train_params)\n        log['train_loss'].append(train_loss)\n        log['train_acc'].append(train_acc)\n\n        val_loss, val_acc = val_loop(**val_params)\n        log['val_loss'].append(val_loss)\n        log['val_acc'].append(val_acc)\n        saving_model(model, optimizer, model_name)\n    \n    export_figure(log)\n    return log\n\n\nreport = report(model, model_name, dataloaders, learning_rate, epoch, 0.001)","metadata":{"execution":{"iopub.status.busy":"2022-10-13T16:47:02.139291Z","iopub.execute_input":"2022-10-13T16:47:02.139913Z","iopub.status.idle":"2022-10-13T17:03:16.304332Z","shell.execute_reply.started":"2022-10-13T16:47:02.13979Z","shell.execute_reply":"2022-10-13T17:03:16.302774Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import json\n\nwith open(\"log.json\", \"w\") as outfile:\n    json.dump(report, outfile)","metadata":{"execution":{"iopub.status.busy":"2022-10-13T17:03:16.305646Z","iopub.status.idle":"2022-10-13T17:03:16.306398Z","shell.execute_reply.started":"2022-10-13T17:03:16.306136Z","shell.execute_reply":"2022-10-13T17:03:16.30616Z"},"trusted":true},"execution_count":null,"outputs":[]}]}